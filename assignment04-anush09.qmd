---
title: Assignment 04
author:
  - name: Anu Sharma
    affiliations:
      - id: anush09
        name: Boston University
        city: Boston
        state: MA
number-sections: true
date: '2025-10-03'
date-modified: today
date-format: long
format:
  html:
    theme: cerulean
    toc: true
    toc-depth: 2

execute:
  echo: false
  eval: false
  freeze: auto
---

```{python}
#| eval: true
#| echo: true
#| output: true
from pyspark.sql import SparkSession
import pandas as pd
import plotly.express as px
import plotly.io as pio
import numpy as np

np.random.seed(42)

pio.renderers.default = "notebook+notebook_connected+vscode"

# Initialize Spark Session
spark = SparkSession.builder.appName("LightcastData").getOrCreate()

# Load Data
df = spark.read.option("header", "true").option("inferSchema", "true").option("multiLine","true").option("escape", "\"").csv("./data/lightcast_job_postings.csv")

# # Show Schema and Sample Data
# print("---This is Diagnostic check, No need to print it in the final doc---")

# # df.printSchema() # comment this line when rendering the submission
# df.show(5)
```

## Feature Engineering
```{python}
#| eval: true
#| echo: true
#| output: true

from pyspark.sql.functions import col, pow
from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler
from pyspark.ml import Pipeline
from pyspark.ml.regression import LinearRegression

# Drop missing values
key_features = [
    "DURATION", "SALARY_FROM", "SALARY_TO", "MIN_YEARS_EXPERIENCE",
    "EMPLOYMENT_TYPE_NAME", "STATE_NAME", "SALARY"
]
df_clean = df.dropna(subset=key_features)

# Categorical transformations
cat_cols = ["EMPLOYMENT_TYPE_NAME", "STATE_NAME"]
indexers = [StringIndexer(inputCol=c, outputCol=c+"_IDX", handleInvalid="keep") for c in cat_cols]
encoders = [OneHotEncoder(inputCol=c+"_IDX", outputCol=c+"_OHE", dropLast=True) for c in cat_cols]

#  Assemble features
cont_cols = ["DURATION", "SALARY_FROM", "SALARY_TO", "MIN_YEARS_EXPERIENCE"]
assembler_inputs = cont_cols + [c+"_OHE" for c in cat_cols]

assembler = VectorAssembler(
    inputCols=assembler_inputs,
    outputCol="features"
)

pipeline = Pipeline(stages=indexers + encoders + [assembler])
df_transformed = pipeline.fit(df_clean).transform(df_clean)

# Train-test split (80/20)
train_df, test_df = df_transformed.randomSplit([0.8, 0.2], seed=42)

# Create polynomial feature ( by squaring MIN_YEARS_EXPERIENCE)
df_poly = df_transformed.withColumn(
    "MIN_YEARS_EXPERIENCE_SQ", pow(col("MIN_YEARS_EXPERIENCE"), 2)
)

# Assemble polynomial features into new vector
poly_assembler = VectorAssembler(
    inputCols=["features", "MIN_YEARS_EXPERIENCE_SQ"],
    outputCol="features_poly"
)
df_final = poly_assembler.transform(df_poly) # final structure
df_final.show(5)
```

##  Linear Regression model
```{python}
#| eval: true
#| echo: true
#| output: true
lr = LinearRegression(
    featuresCol="features",
    labelCol="SALARY",
    predictionCol="prediction",
    solver="normal"
)

lr_model = lr.fit(train_df)

test_results = lr_model.evaluate(test_df)

print("Coefficients:", lr_model.coefficients)
print("Intercept:", lr_model.intercept)
print("RÂ²:", test_results.r2)
print("RMSE:", test_results.rootMeanSquaredError)
print("MAE:", test_results.meanAbsoluteError)

# Coefficient statistics
training_summary = lr_model.summary

try:
    coefs = lr_model.coefficients.toArray().tolist()
    se = training_summary.coefficientStandardErrors
    tvals = training_summary.tValues
    pvals = training_summary.pValues

    coef_df = spark.createDataFrame(
        [
            (float(coefs[i]), float(se[i]), float(tvals[i]), float(pvals[i]),
             float(coefs[i] - 1.96*se[i]), float(coefs[i] + 1.96*se[i]))
            for i in range(len(coefs))
        ],
        ["Coefficient", "StdError", "tValue", "pValue", "CI_lower", "CI_upper"]
    )

    coef_df.show(truncate=False)

except Exception as e:
    print("Coefficient statistics not available (L-BGFS fallback):", str(e))

```